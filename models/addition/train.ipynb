{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc1930f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchinfo\n",
    "from model import Transformer\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a9342db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 6, 1],\n",
       "        [0, 3, 6, 3],\n",
       "        [5, 0, 6, 5],\n",
       "        [4, 4, 6, 2],\n",
       "        [2, 2, 6, 4],\n",
       "        [3, 5, 6, 2],\n",
       "        [2, 4, 6, 0],\n",
       "        [5, 4, 6, 3],\n",
       "        [3, 4, 6, 1],\n",
       "        [1, 5, 6, 0],\n",
       "        [0, 4, 6, 4],\n",
       "        [4, 1, 6, 5],\n",
       "        [1, 4, 6, 5],\n",
       "        [5, 5, 6, 4],\n",
       "        [4, 3, 6, 1],\n",
       "        [5, 1, 6, 0],\n",
       "        [2, 3, 6, 5],\n",
       "        [1, 3, 6, 4]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"../../datasets/addition/dataset_train.pt\", \"rb\") as f:\n",
    "    train_BT = torch.load(f, weights_only=False)\n",
    "\n",
    "with open(\"../../datasets/addition/dataset_test.pt\", \"rb\") as f:\n",
    "    test_BT = torch.load(f, weights_only=False)\n",
    "\n",
    "train_BT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9beea426-a309-4995-9706-dcb8af78dbd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = test_BT[0, 2].item() + 1\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d702741-4696-446b-99c6-532503b37c74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (embedding): Embedding(7, 3)\n",
       "  (encoder_layers): ModuleList(\n",
       "    (0): TransformerEncoderLayer(\n",
       "      (self_attn): MultiHeadSelfAttention(\n",
       "        (q_proj): Linear(in_features=3, out_features=3, bias=True)\n",
       "        (k_proj): Linear(in_features=3, out_features=3, bias=True)\n",
       "        (v_proj): Linear(in_features=3, out_features=3, bias=True)\n",
       "        (o_proj): Linear(in_features=3, out_features=3, bias=True)\n",
       "      )\n",
       "      (ff): PositionwiseFeedForward(\n",
       "        (fc1): Linear(in_features=3, out_features=12, bias=True)\n",
       "        (fc2): Linear(in_features=12, out_features=3, bias=True)\n",
       "      )\n",
       "      (ln1): LayerNorm((3,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((3,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (fc_out): Linear(in_features=3, out_features=7, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Transformer(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=3,\n",
    "    n_heads=1,\n",
    "    layers=1\n",
    ")\n",
    "model.compile()\n",
    "\n",
    "torchinfo.summary(model)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2092a1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=0.0005\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bccdf76e-4fac-4753-91b6-182b00ce8eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100 | Train Loss: 1.8276 | Val Loss: 1.8290 | Val Acc: 0.3750\n",
      "Step: 200 | Train Loss: 1.5846 | Val Loss: 1.6150 | Val Acc: 0.3750\n",
      "Step: 300 | Train Loss: 1.4048 | Val Loss: 1.4479 | Val Acc: 0.4750\n",
      "Step: 400 | Train Loss: 1.2882 | Val Loss: 1.3349 | Val Acc: 0.5500\n",
      "Step: 500 | Train Loss: 1.1646 | Val Loss: 1.2143 | Val Acc: 0.6500\n",
      "Step: 600 | Train Loss: 1.0121 | Val Loss: 1.0552 | Val Acc: 0.7500\n",
      "Step: 700 | Train Loss: 0.8966 | Val Loss: 0.9346 | Val Acc: 0.8000\n",
      "Step: 800 | Train Loss: 0.8084 | Val Loss: 0.8445 | Val Acc: 0.8000\n",
      "Step: 900 | Train Loss: 0.7349 | Val Loss: 0.7697 | Val Acc: 0.8000\n",
      "Step: 1000 | Train Loss: 0.6727 | Val Loss: 0.7067 | Val Acc: 0.8000\n",
      "Step: 1100 | Train Loss: 0.6193 | Val Loss: 0.6531 | Val Acc: 0.8000\n",
      "Step: 1200 | Train Loss: 0.5727 | Val Loss: 0.6068 | Val Acc: 0.8500\n",
      "Step: 1300 | Train Loss: 0.5315 | Val Loss: 0.5657 | Val Acc: 0.8500\n",
      "Step: 1400 | Train Loss: 0.4940 | Val Loss: 0.5281 | Val Acc: 1.0000\n",
      "Step: 1500 | Train Loss: 0.4591 | Val Loss: 0.4922 | Val Acc: 1.0000\n",
      "Step: 1600 | Train Loss: 0.4258 | Val Loss: 0.4567 | Val Acc: 1.0000\n",
      "Step: 1700 | Train Loss: 0.3938 | Val Loss: 0.4215 | Val Acc: 1.0000\n",
      "Step: 1800 | Train Loss: 0.3632 | Val Loss: 0.3873 | Val Acc: 1.0000\n",
      "Step: 1900 | Train Loss: 0.3345 | Val Loss: 0.3550 | Val Acc: 1.0000\n",
      "Step: 2000 | Train Loss: 0.3078 | Val Loss: 0.3251 | Val Acc: 1.0000\n",
      "Step: 2100 | Train Loss: 0.2832 | Val Loss: 0.2977 | Val Acc: 1.0000\n",
      "Step: 2200 | Train Loss: 0.2604 | Val Loss: 0.2728 | Val Acc: 1.0000\n",
      "Step: 2300 | Train Loss: 0.2395 | Val Loss: 0.2501 | Val Acc: 1.0000\n",
      "Step: 2400 | Train Loss: 0.2202 | Val Loss: 0.2293 | Val Acc: 1.0000\n",
      "Step: 2500 | Train Loss: 0.2024 | Val Loss: 0.2103 | Val Acc: 1.0000\n",
      "Step: 2600 | Train Loss: 0.1861 | Val Loss: 0.1930 | Val Acc: 1.0000\n",
      "Step: 2700 | Train Loss: 0.1711 | Val Loss: 0.1772 | Val Acc: 1.0000\n",
      "Step: 2800 | Train Loss: 0.1574 | Val Loss: 0.1627 | Val Acc: 1.0000\n",
      "Step: 2900 | Train Loss: 0.1448 | Val Loss: 0.1495 | Val Acc: 1.0000\n",
      "Step: 3000 | Train Loss: 0.1333 | Val Loss: 0.1374 | Val Acc: 1.0000\n",
      "Step: 3100 | Train Loss: 0.1227 | Val Loss: 0.1263 | Val Acc: 1.0000\n",
      "Step: 3200 | Train Loss: 0.1130 | Val Loss: 0.1162 | Val Acc: 1.0000\n",
      "Step: 3300 | Train Loss: 0.1042 | Val Loss: 0.1070 | Val Acc: 1.0000\n",
      "Step: 3400 | Train Loss: 0.0961 | Val Loss: 0.0986 | Val Acc: 1.0000\n",
      "Step: 3500 | Train Loss: 0.0887 | Val Loss: 0.0910 | Val Acc: 1.0000\n",
      "Step: 3600 | Train Loss: 0.0819 | Val Loss: 0.0839 | Val Acc: 1.0000\n",
      "Step: 3700 | Train Loss: 0.0757 | Val Loss: 0.0775 | Val Acc: 1.0000\n",
      "Step: 3800 | Train Loss: 0.0701 | Val Loss: 0.0716 | Val Acc: 1.0000\n",
      "Step: 3900 | Train Loss: 0.0649 | Val Loss: 0.0663 | Val Acc: 1.0000\n",
      "Step: 4000 | Train Loss: 0.0601 | Val Loss: 0.0613 | Val Acc: 1.0000\n",
      "Step: 4100 | Train Loss: 0.0557 | Val Loss: 0.0568 | Val Acc: 1.0000\n",
      "Step: 4200 | Train Loss: 0.0517 | Val Loss: 0.0527 | Val Acc: 1.0000\n",
      "Step: 4300 | Train Loss: 0.0480 | Val Loss: 0.0489 | Val Acc: 1.0000\n",
      "Step: 4400 | Train Loss: 0.0446 | Val Loss: 0.0454 | Val Acc: 1.0000\n",
      "Step: 4500 | Train Loss: 0.0414 | Val Loss: 0.0422 | Val Acc: 1.0000\n",
      "Step: 4600 | Train Loss: 0.0386 | Val Loss: 0.0392 | Val Acc: 1.0000\n",
      "Step: 4700 | Train Loss: 0.0359 | Val Loss: 0.0365 | Val Acc: 1.0000\n",
      "Step: 4800 | Train Loss: 0.0334 | Val Loss: 0.0340 | Val Acc: 1.0000\n",
      "Step: 4900 | Train Loss: 0.0312 | Val Loss: 0.0316 | Val Acc: 1.0000\n",
      "Step: 5000 | Train Loss: 0.0291 | Val Loss: 0.0295 | Val Acc: 1.0000\n",
      "Step: 5100 | Train Loss: 0.0271 | Val Loss: 0.0275 | Val Acc: 1.0000\n",
      "Step: 5200 | Train Loss: 0.0253 | Val Loss: 0.0257 | Val Acc: 1.0000\n",
      "Step: 5300 | Train Loss: 0.0237 | Val Loss: 0.0240 | Val Acc: 1.0000\n",
      "Step: 5400 | Train Loss: 0.0221 | Val Loss: 0.0224 | Val Acc: 1.0000\n",
      "Step: 5500 | Train Loss: 0.0207 | Val Loss: 0.0209 | Val Acc: 1.0000\n",
      "Step: 5600 | Train Loss: 0.0194 | Val Loss: 0.0196 | Val Acc: 1.0000\n",
      "Step: 5700 | Train Loss: 0.0181 | Val Loss: 0.0183 | Val Acc: 1.0000\n",
      "Step: 5800 | Train Loss: 0.0170 | Val Loss: 0.0172 | Val Acc: 1.0000\n",
      "Step: 5900 | Train Loss: 0.0159 | Val Loss: 0.0161 | Val Acc: 1.0000\n",
      "Step: 6000 | Train Loss: 0.0149 | Val Loss: 0.0151 | Val Acc: 1.0000\n",
      "Step: 6100 | Train Loss: 0.0140 | Val Loss: 0.0141 | Val Acc: 1.0000\n",
      "Step: 6200 | Train Loss: 0.0131 | Val Loss: 0.0132 | Val Acc: 1.0000\n",
      "Step: 6300 | Train Loss: 0.0123 | Val Loss: 0.0124 | Val Acc: 1.0000\n",
      "Step: 6400 | Train Loss: 0.0116 | Val Loss: 0.0117 | Val Acc: 1.0000\n",
      "Step: 6500 | Train Loss: 0.0109 | Val Loss: 0.0109 | Val Acc: 1.0000\n",
      "Step: 6600 | Train Loss: 0.0102 | Val Loss: 0.0103 | Val Acc: 1.0000\n",
      "Step: 6700 | Train Loss: 0.0096 | Val Loss: 0.0097 | Val Acc: 1.0000\n",
      "Step: 6800 | Train Loss: 0.0090 | Val Loss: 0.0091 | Val Acc: 1.0000\n",
      "Step: 6900 | Train Loss: 0.0085 | Val Loss: 0.0085 | Val Acc: 1.0000\n",
      "Step: 7000 | Train Loss: 0.0080 | Val Loss: 0.0080 | Val Acc: 1.0000\n",
      "Step: 7100 | Train Loss: 0.0075 | Val Loss: 0.0076 | Val Acc: 1.0000\n",
      "Step: 7200 | Train Loss: 0.0071 | Val Loss: 0.0071 | Val Acc: 1.0000\n",
      "Step: 7300 | Train Loss: 0.0067 | Val Loss: 0.0067 | Val Acc: 1.0000\n",
      "Step: 7400 | Train Loss: 0.0063 | Val Loss: 0.0063 | Val Acc: 1.0000\n",
      "Step: 7500 | Train Loss: 0.0059 | Val Loss: 0.0059 | Val Acc: 1.0000\n",
      "Step: 7600 | Train Loss: 0.0056 | Val Loss: 0.0056 | Val Acc: 1.0000\n",
      "Step: 7700 | Train Loss: 0.0053 | Val Loss: 0.0053 | Val Acc: 1.0000\n",
      "Step: 7800 | Train Loss: 0.0050 | Val Loss: 0.0050 | Val Acc: 1.0000\n",
      "Step: 7900 | Train Loss: 0.0047 | Val Loss: 0.0047 | Val Acc: 1.0000\n",
      "Step: 8000 | Train Loss: 0.0044 | Val Loss: 0.0044 | Val Acc: 1.0000\n",
      "Step: 8100 | Train Loss: 0.0042 | Val Loss: 0.0042 | Val Acc: 1.0000\n",
      "Step: 8200 | Train Loss: 0.0039 | Val Loss: 0.0039 | Val Acc: 1.0000\n",
      "Step: 8300 | Train Loss: 0.0037 | Val Loss: 0.0037 | Val Acc: 1.0000\n",
      "Step: 8400 | Train Loss: 0.0035 | Val Loss: 0.0035 | Val Acc: 1.0000\n",
      "Step: 8500 | Train Loss: 0.0033 | Val Loss: 0.0033 | Val Acc: 1.0000\n",
      "Step: 8600 | Train Loss: 0.0031 | Val Loss: 0.0031 | Val Acc: 1.0000\n",
      "Step: 8700 | Train Loss: 0.0029 | Val Loss: 0.0029 | Val Acc: 1.0000\n",
      "Step: 8800 | Train Loss: 0.0028 | Val Loss: 0.0028 | Val Acc: 1.0000\n",
      "Step: 8900 | Train Loss: 0.0026 | Val Loss: 0.0026 | Val Acc: 1.0000\n",
      "Step: 9000 | Train Loss: 0.0025 | Val Loss: 0.0025 | Val Acc: 1.0000\n",
      "Step: 9100 | Train Loss: 0.0023 | Val Loss: 0.0023 | Val Acc: 1.0000\n",
      "Step: 9200 | Train Loss: 0.0022 | Val Loss: 0.0022 | Val Acc: 1.0000\n",
      "Step: 9300 | Train Loss: 0.0021 | Val Loss: 0.0021 | Val Acc: 1.0000\n",
      "Step: 9400 | Train Loss: 0.0020 | Val Loss: 0.0020 | Val Acc: 1.0000\n",
      "Step: 9500 | Train Loss: 0.0019 | Val Loss: 0.0019 | Val Acc: 1.0000\n",
      "Step: 9600 | Train Loss: 0.0018 | Val Loss: 0.0018 | Val Acc: 1.0000\n",
      "Step: 9700 | Train Loss: 0.0017 | Val Loss: 0.0017 | Val Acc: 1.0000\n",
      "Step: 9800 | Train Loss: 0.0016 | Val Loss: 0.0016 | Val Acc: 1.0000\n",
      "Step: 9900 | Train Loss: 0.0015 | Val Loss: 0.0015 | Val Acc: 1.0000\n",
      "Step: 10000 | Train Loss: 0.0014 | Val Loss: 0.0014 | Val Acc: 1.0000\n",
      "Step: 10100 | Train Loss: 0.0013 | Val Loss: 0.0013 | Val Acc: 1.0000\n",
      "Step: 10200 | Train Loss: 0.0013 | Val Loss: 0.0013 | Val Acc: 1.0000\n",
      "Step: 10300 | Train Loss: 0.0012 | Val Loss: 0.0012 | Val Acc: 1.0000\n",
      "Step: 10400 | Train Loss: 0.0011 | Val Loss: 0.0011 | Val Acc: 1.0000\n",
      "Step: 10500 | Train Loss: 0.0011 | Val Loss: 0.0011 | Val Acc: 1.0000\n",
      "Step: 10600 | Train Loss: 0.0010 | Val Loss: 0.0010 | Val Acc: 1.0000\n",
      "Step: 10700 | Train Loss: 0.0010 | Val Loss: 0.0010 | Val Acc: 1.0000\n",
      "Step: 10800 | Train Loss: 0.0009 | Val Loss: 0.0009 | Val Acc: 1.0000\n",
      "Step: 10900 | Train Loss: 0.0009 | Val Loss: 0.0009 | Val Acc: 1.0000\n",
      "Step: 11000 | Train Loss: 0.0008 | Val Loss: 0.0008 | Val Acc: 1.0000\n",
      "Step: 11100 | Train Loss: 0.0008 | Val Loss: 0.0008 | Val Acc: 1.0000\n",
      "Step: 11200 | Train Loss: 0.0007 | Val Loss: 0.0007 | Val Acc: 1.0000\n",
      "Step: 11300 | Train Loss: 0.0007 | Val Loss: 0.0007 | Val Acc: 1.0000\n",
      "Step: 11400 | Train Loss: 0.0007 | Val Loss: 0.0007 | Val Acc: 1.0000\n",
      "Step: 11500 | Train Loss: 0.0006 | Val Loss: 0.0006 | Val Acc: 1.0000\n",
      "Step: 11600 | Train Loss: 0.0006 | Val Loss: 0.0006 | Val Acc: 1.0000\n",
      "Step: 11700 | Train Loss: 0.0006 | Val Loss: 0.0006 | Val Acc: 1.0000\n",
      "Step: 11800 | Train Loss: 0.0005 | Val Loss: 0.0005 | Val Acc: 1.0000\n",
      "Step: 11900 | Train Loss: 0.0005 | Val Loss: 0.0005 | Val Acc: 1.0000\n",
      "Step: 12000 | Train Loss: 0.0005 | Val Loss: 0.0005 | Val Acc: 1.0000\n",
      "Step: 12100 | Train Loss: 0.0004 | Val Loss: 0.0004 | Val Acc: 1.0000\n",
      "Step: 12200 | Train Loss: 0.0004 | Val Loss: 0.0004 | Val Acc: 1.0000\n",
      "Step: 12300 | Train Loss: 0.0004 | Val Loss: 0.0004 | Val Acc: 1.0000\n",
      "Step: 12400 | Train Loss: 0.0004 | Val Loss: 0.0004 | Val Acc: 1.0000\n",
      "Step: 12500 | Train Loss: 0.0004 | Val Loss: 0.0004 | Val Acc: 1.0000\n",
      "Step: 12600 | Train Loss: 0.0003 | Val Loss: 0.0003 | Val Acc: 1.0000\n",
      "Step: 12700 | Train Loss: 0.0003 | Val Loss: 0.0003 | Val Acc: 1.0000\n",
      "Step: 12800 | Train Loss: 0.0003 | Val Loss: 0.0003 | Val Acc: 1.0000\n",
      "Step: 12900 | Train Loss: 0.0003 | Val Loss: 0.0003 | Val Acc: 1.0000\n",
      "Step: 13000 | Train Loss: 0.0003 | Val Loss: 0.0003 | Val Acc: 1.0000\n",
      "Step: 13100 | Train Loss: 0.0003 | Val Loss: 0.0003 | Val Acc: 1.0000\n",
      "Step: 13200 | Train Loss: 0.0002 | Val Loss: 0.0002 | Val Acc: 1.0000\n",
      "Step: 13300 | Train Loss: 0.0002 | Val Loss: 0.0002 | Val Acc: 1.0000\n",
      "Step: 13400 | Train Loss: 0.0002 | Val Loss: 0.0002 | Val Acc: 1.0000\n",
      "Step: 13500 | Train Loss: 0.0002 | Val Loss: 0.0002 | Val Acc: 1.0000\n",
      "Step: 13600 | Train Loss: 0.0002 | Val Loss: 0.0002 | Val Acc: 1.0000\n",
      "Step: 13700 | Train Loss: 0.0002 | Val Loss: 0.0002 | Val Acc: 1.0000\n",
      "Step: 13800 | Train Loss: 0.0002 | Val Loss: 0.0002 | Val Acc: 1.0000\n",
      "Step: 13900 | Train Loss: 0.0002 | Val Loss: 0.0002 | Val Acc: 1.0000\n",
      "Step: 14000 | Train Loss: 0.0002 | Val Loss: 0.0002 | Val Acc: 1.0000\n",
      "Step: 14100 | Train Loss: 0.0002 | Val Loss: 0.0002 | Val Acc: 1.0000\n",
      "Step: 14200 | Train Loss: 0.0001 | Val Loss: 0.0001 | Val Acc: 1.0000\n",
      "Step: 14300 | Train Loss: 0.0001 | Val Loss: 0.0001 | Val Acc: 1.0000\n",
      "Step: 14400 | Train Loss: 0.0001 | Val Loss: 0.0001 | Val Acc: 1.0000\n",
      "Step: 14500 | Train Loss: 0.0001 | Val Loss: 0.0001 | Val Acc: 1.0000\n",
      "Step: 14600 | Train Loss: 0.0001 | Val Loss: 0.0001 | Val Acc: 1.0000\n",
      "Step: 14700 | Train Loss: 0.0001 | Val Loss: 0.0001 | Val Acc: 1.0000\n",
      "Step: 14800 | Train Loss: 0.0001 | Val Loss: 0.0001 | Val Acc: 1.0000\n",
      "Step: 14900 | Train Loss: 0.0001 | Val Loss: 0.0001 | Val Acc: 1.0000\n",
      "Step: 15000 | Train Loss: 0.0001 | Val Loss: 0.0001 | Val Acc: 1.0000\n",
      "Step: 15100 | Train Loss: 0.0001 | Val Loss: 0.0001 | Val Acc: 1.0000\n",
      "Step: 15200 | Train Loss: 0.0001 | Val Loss: 0.0001 | Val Acc: 1.0000\n",
      "Step: 15300 | Train Loss: 0.0001 | Val Loss: 0.0001 | Val Acc: 1.0000\n",
      "Step: 15400 | Train Loss: 0.0001 | Val Loss: 0.0001 | Val Acc: 1.0000\n",
      "Step: 15500 | Train Loss: 0.0001 | Val Loss: 0.0001 | Val Acc: 1.0000\n",
      "Step: 15600 | Train Loss: 0.0001 | Val Loss: 0.0001 | Val Acc: 1.0000\n",
      "Step: 15700 | Train Loss: 0.0001 | Val Loss: 0.0001 | Val Acc: 1.0000\n",
      "Step: 15800 | Train Loss: 0.0001 | Val Loss: 0.0001 | Val Acc: 1.0000\n",
      "Step: 15900 | Train Loss: 0.0001 | Val Loss: 0.0001 | Val Acc: 1.0000\n",
      "Step: 16000 | Train Loss: 0.0001 | Val Loss: 0.0001 | Val Acc: 1.0000\n",
      "Step: 16100 | Train Loss: 0.0001 | Val Loss: 0.0001 | Val Acc: 1.0000\n",
      "Step: 16200 | Train Loss: 0.0001 | Val Loss: 0.0001 | Val Acc: 1.0000\n",
      "Step: 16300 | Train Loss: 0.0000 | Val Loss: 0.0000 | Val Acc: 1.0000\n",
      "Step: 16400 | Train Loss: 0.0000 | Val Loss: 0.0000 | Val Acc: 1.0000\n",
      "Step: 16500 | Train Loss: 0.0000 | Val Loss: 0.0000 | Val Acc: 1.0000\n",
      "Step: 16600 | Train Loss: 0.0000 | Val Loss: 0.0000 | Val Acc: 1.0000\n",
      "Step: 16700 | Train Loss: 0.0000 | Val Loss: 0.0000 | Val Acc: 1.0000\n",
      "Step: 16800 | Train Loss: 0.0000 | Val Loss: 0.0000 | Val Acc: 1.0000\n",
      "Step: 16900 | Train Loss: 0.0000 | Val Loss: 0.0000 | Val Acc: 1.0000\n",
      "Step: 17000 | Train Loss: 0.0000 | Val Loss: 0.0000 | Val Acc: 1.0000\n",
      "Step: 17100 | Train Loss: 0.0000 | Val Loss: 0.0000 | Val Acc: 1.0000\n",
      "Step: 17200 | Train Loss: 0.0000 | Val Loss: 0.0000 | Val Acc: 1.0000\n",
      "Step: 17300 | Train Loss: 0.0000 | Val Loss: 0.0000 | Val Acc: 1.0000\n",
      "Step: 17400 | Train Loss: 0.0000 | Val Loss: 0.0000 | Val Acc: 1.0000\n",
      "Step: 17500 | Train Loss: 0.0000 | Val Loss: 0.0000 | Val Acc: 1.0000\n",
      "Step: 17600 | Train Loss: 0.0000 | Val Loss: 0.0000 | Val Acc: 1.0000\n",
      "Step: 17700 | Train Loss: 0.0000 | Val Loss: 0.0000 | Val Acc: 1.0000\n",
      "Step: 17800 | Train Loss: 0.0000 | Val Loss: 0.0000 | Val Acc: 1.0000\n",
      "Step: 17900 | Train Loss: 0.0000 | Val Loss: 0.0000 | Val Acc: 1.0000\n",
      "Step: 18000 | Train Loss: 0.0000 | Val Loss: 0.0000 | Val Acc: 1.0000\n",
      "Step: 18100 | Train Loss: 0.0000 | Val Loss: 0.0000 | Val Acc: 1.0000\n",
      "Step: 18200 | Train Loss: 0.0000 | Val Loss: 0.0000 | Val Acc: 1.0000\n",
      "Step: 18300 | Train Loss: 0.0000 | Val Loss: 0.0000 | Val Acc: 1.0000\n",
      "Step: 18400 | Train Loss: 0.0000 | Val Loss: 0.0000 | Val Acc: 1.0000\n",
      "Step: 18500 | Train Loss: 0.0000 | Val Loss: 0.0000 | Val Acc: 1.0000\n",
      "Step: 18600 | Train Loss: 0.0000 | Val Loss: 0.0000 | Val Acc: 1.0000\n",
      "Step: 18700 | Train Loss: 0.0000 | Val Loss: 0.0000 | Val Acc: 1.0000\n",
      "Step: 18800 | Train Loss: 0.0000 | Val Loss: 0.0000 | Val Acc: 1.0000\n",
      "Step: 18900 | Train Loss: 0.0000 | Val Loss: 0.0000 | Val Acc: 1.0000\n",
      "Step: 19000 | Train Loss: 0.0000 | Val Loss: 0.0000 | Val Acc: 1.0000\n",
      "Step: 19100 | Train Loss: 0.0000 | Val Loss: 0.0000 | Val Acc: 1.0000\n",
      "Step: 19200 | Train Loss: 0.0000 | Val Loss: 0.0000 | Val Acc: 1.0000\n",
      "Step: 19300 | Train Loss: 0.0000 | Val Loss: 0.0000 | Val Acc: 1.0000\n",
      "Step: 19400 | Train Loss: 0.0000 | Val Loss: 0.0000 | Val Acc: 1.0000\n",
      "Step: 19500 | Train Loss: 0.0000 | Val Loss: 0.0000 | Val Acc: 1.0000\n",
      "Step: 19600 | Train Loss: 0.0000 | Val Loss: 0.0000 | Val Acc: 1.0000\n",
      "Step: 19700 | Train Loss: 0.0000 | Val Loss: 0.0000 | Val Acc: 1.0000\n",
      "Step: 19800 | Train Loss: 0.0000 | Val Loss: 0.0000 | Val Acc: 1.0000\n",
      "Step: 19900 | Train Loss: 0.0000 | Val Loss: 0.0000 | Val Acc: 1.0000\n",
      "Step: 20000 | Train Loss: 0.0000 | Val Loss: 0.0000 | Val Acc: 1.0000\n",
      "Step: 20100 | Train Loss: 0.0000 | Val Loss: 0.0000 | Val Acc: 1.0000\n",
      "Step: 20200 | Train Loss: 0.0000 | Val Loss: 0.0000 | Val Acc: 1.0000\n",
      "Step: 20300 | Train Loss: 0.0000 | Val Loss: 0.0000 | Val Acc: 1.0000\n",
      "Step: 20400 | Train Loss: 0.0000 | Val Loss: 0.0000 | Val Acc: 1.0000\n",
      "Step: 20500 | Train Loss: 0.0000 | Val Loss: 0.0000 | Val Acc: 1.0000\n",
      "Step: 20600 | Train Loss: 0.0000 | Val Loss: 0.0000 | Val Acc: 1.0000\n",
      "Step: 20700 | Train Loss: 0.0000 | Val Loss: 0.0000 | Val Acc: 1.0000\n",
      "Step: 20800 | Train Loss: 0.0000 | Val Loss: 0.0000 | Val Acc: 1.0000\n",
      "Step: 20900 | Train Loss: 0.0000 | Val Loss: 0.0000 | Val Acc: 1.0000\n",
      "Step: 21000 | Train Loss: 0.0000 | Val Loss: 0.0000 | Val Acc: 1.0000\n",
      "Step: 21100 | Train Loss: 0.0000 | Val Loss: 0.0000 | Val Acc: 1.0000\n",
      "Step: 21200 | Train Loss: 0.0000 | Val Loss: 0.0000 | Val Acc: 1.0000\n",
      "Step: 21300 | Train Loss: 0.0000 | Val Loss: 0.0000 | Val Acc: 1.0000\n",
      "Step: 21400 | Train Loss: 0.0000 | Val Loss: 0.0000 | Val Acc: 1.0000\n",
      "Step: 21500 | Train Loss: 0.0000 | Val Loss: 0.0000 | Val Acc: 1.0000\n",
      "Step: 21600 | Train Loss: 0.0000 | Val Loss: 0.0000 | Val Acc: 1.0000\n",
      "Step: 21700 | Train Loss: 0.0000 | Val Loss: 0.0000 | Val Acc: 1.0000\n",
      "Step: 21800 | Train Loss: 0.0000 | Val Loss: 0.0000 | Val Acc: 1.0000\n",
      "Step: 21900 | Train Loss: 0.0000 | Val Loss: 0.0000 | Val Acc: 1.0000\n",
      "Step: 22000 | Train Loss: 0.0000 | Val Loss: 0.0000 | Val Acc: 1.0000\n",
      "Step: 22100 | Train Loss: 0.0000 | Val Loss: 0.0000 | Val Acc: 1.0000\n",
      "Step: 22200 | Train Loss: 0.0000 | Val Loss: 0.0000 | Val Acc: 1.0000\n",
      "Step: 22300 | Train Loss: 0.0000 | Val Loss: 0.0000 | Val Acc: 1.0000\n",
      "Step: 22400 | Train Loss: 0.0000 | Val Loss: 0.0000 | Val Acc: 1.0000\n",
      "Step: 22500 | Train Loss: 0.0000 | Val Loss: 0.0000 | Val Acc: 1.0000\n",
      "Step: 22600 | Train Loss: 0.0000 | Val Loss: 0.0000 | Val Acc: 1.0000\n",
      "Step: 22700 | Train Loss: 0.0000 | Val Loss: 0.0000 | Val Acc: 1.0000\n",
      "Step: 22800 | Train Loss: 0.0000 | Val Loss: 0.0000 | Val Acc: 1.0000\n",
      "Step: 22900 | Train Loss: 0.0000 | Val Loss: 0.0000 | Val Acc: 1.0000\n",
      "Step: 23000 | Train Loss: 0.0000 | Val Loss: 0.0000 | Val Acc: 1.0000\n",
      "Step: 23100 | Train Loss: 0.0000 | Val Loss: 0.0000 | Val Acc: 1.0000\n",
      "Step: 23200 | Train Loss: 0.0000 | Val Loss: 0.0000 | Val Acc: 1.0000\n",
      "Step: 23300 | Train Loss: 0.0000 | Val Loss: 0.0000 | Val Acc: 1.0000\n",
      "Step: 23400 | Train Loss: 0.0000 | Val Loss: 0.0000 | Val Acc: 1.0000\n",
      "Step: 23500 | Train Loss: 0.0000 | Val Loss: 0.0000 | Val Acc: 1.0000\n",
      "Step: 23600 | Train Loss: 0.0000 | Val Loss: 0.0000 | Val Acc: 1.0000\n",
      "Step: 23700 | Train Loss: 0.0000 | Val Loss: 0.0000 | Val Acc: 1.0000\n",
      "Step: 23800 | Train Loss: 0.0000 | Val Loss: 0.0000 | Val Acc: 1.0000\n",
      "Step: 23900 | Train Loss: 0.0000 | Val Loss: 0.0000 | Val Acc: 1.0000\n",
      "Step: 24000 | Train Loss: 0.0000 | Val Loss: 0.0000 | Val Acc: 1.0000\n",
      "Step: 24100 | Train Loss: 0.0000 | Val Loss: 0.0000 | Val Acc: 1.0000\n",
      "Step: 24200 | Train Loss: 0.0000 | Val Loss: 0.0000 | Val Acc: 1.0000\n",
      "Step: 24300 | Train Loss: 0.0000 | Val Loss: 0.0000 | Val Acc: 1.0000\n",
      "Step: 24400 | Train Loss: 0.0000 | Val Loss: 0.0000 | Val Acc: 1.0000\n",
      "Step: 24500 | Train Loss: 0.0000 | Val Loss: 0.0000 | Val Acc: 1.0000\n",
      "Step: 24600 | Train Loss: 0.0000 | Val Loss: 0.0000 | Val Acc: 1.0000\n",
      "Step: 24700 | Train Loss: 0.0000 | Val Loss: 0.0000 | Val Acc: 1.0000\n",
      "Step: 24800 | Train Loss: 0.0000 | Val Loss: 0.0000 | Val Acc: 1.0000\n",
      "Step: 24900 | Train Loss: 0.0000 | Val Loss: 0.0000 | Val Acc: 1.0000\n",
      "Step: 25000 | Train Loss: 0.0000 | Val Loss: 0.0000 | Val Acc: 1.0000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     16\u001b[39m loss = criterion(pred_flat, batch_flat)\n\u001b[32m     18\u001b[39m loss.backward()\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m optimizer.zero_grad()\n\u001b[32m     22\u001b[39m current_step += \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/nix/store/g3qvln8r7xzbqv04syri3v7vc5y9fg0c-python3-3.13.9-env/lib/python3.13/site-packages/torch/optim/optimizer.py:517\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    512\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    513\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    514\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    515\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m517\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    518\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    520\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/nix/store/g3qvln8r7xzbqv04syri3v7vc5y9fg0c-python3-3.13.9-env/lib/python3.13/site-packages/torch/optim/optimizer.py:82\u001b[39m, in \u001b[36m_use_grad_for_differentiable.<locals>._use_grad\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     80\u001b[39m     torch.set_grad_enabled(\u001b[38;5;28mself\u001b[39m.defaults[\u001b[33m\"\u001b[39m\u001b[33mdifferentiable\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     81\u001b[39m     torch._dynamo.graph_break()\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     84\u001b[39m     torch._dynamo.graph_break()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/nix/store/g3qvln8r7xzbqv04syri3v7vc5y9fg0c-python3-3.13.9-env/lib/python3.13/site-packages/torch/optim/adam.py:247\u001b[39m, in \u001b[36mAdam.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    235\u001b[39m     beta1, beta2 = group[\u001b[33m\"\u001b[39m\u001b[33mbetas\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    237\u001b[39m     has_complex = \u001b[38;5;28mself\u001b[39m._init_group(\n\u001b[32m    238\u001b[39m         group,\n\u001b[32m    239\u001b[39m         params_with_grad,\n\u001b[32m   (...)\u001b[39m\u001b[32m    244\u001b[39m         state_steps,\n\u001b[32m    245\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m247\u001b[39m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mamsgrad\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43meps\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmaximize\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mforeach\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcapturable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdifferentiable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    265\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfused\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgrad_scale\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfound_inf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdecoupled_weight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    269\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/nix/store/g3qvln8r7xzbqv04syri3v7vc5y9fg0c-python3-3.13.9-env/lib/python3.13/site-packages/torch/optim/optimizer.py:150\u001b[39m, in \u001b[36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(*args, **kwargs)\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/nix/store/g3qvln8r7xzbqv04syri3v7vc5y9fg0c-python3-3.13.9-env/lib/python3.13/site-packages/torch/optim/adam.py:953\u001b[39m, in \u001b[36madam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, decoupled_weight_decay, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[39m\n\u001b[32m    950\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    951\u001b[39m     func = _single_tensor_adam\n\u001b[32m--> \u001b[39m\u001b[32m953\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    954\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    955\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    956\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    957\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    958\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    959\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    960\u001b[39m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    961\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    962\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    963\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    964\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    965\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    966\u001b[39m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    967\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    968\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    969\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    970\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    971\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    972\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    973\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/nix/store/g3qvln8r7xzbqv04syri3v7vc5y9fg0c-python3-3.13.9-env/lib/python3.13/site-packages/torch/optim/adam.py:466\u001b[39m, in \u001b[36m_single_tensor_adam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, decoupled_weight_decay)\u001b[39m\n\u001b[32m    462\u001b[39m         exp_avg_sq.mul_(beta2).addcmul_(\n\u001b[32m    463\u001b[39m             grad, grad, value=cast(\u001b[38;5;28mfloat\u001b[39m, \u001b[32m1\u001b[39m - beta2)\n\u001b[32m    464\u001b[39m         )\n\u001b[32m    465\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m466\u001b[39m     \u001b[43mexp_avg_sq\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43maddcmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m    468\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m capturable \u001b[38;5;129;01mor\u001b[39;00m differentiable:\n\u001b[32m    469\u001b[39m     step = step_t\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "total_steps = 100000\n",
    "validation_mod = 100\n",
    "batch_size = 10\n",
    "\n",
    "current_step = 0\n",
    "\n",
    "while current_step < total_steps:\n",
    "    for i in range(train_BT.shape[0] // batch_size):\n",
    "        batch = train_BT[i * batch_size:(i + 1) * batch_size]  # [B, 4]\n",
    "\n",
    "        pred = model(batch)  # [B, 4, vocab_size]\n",
    "\n",
    "        batch_flat = batch.view(-1)                     # [B*4]\n",
    "        pred_flat = pred.view(-1, vocab_size)           # [B*4, V]\n",
    "\n",
    "        loss = criterion(pred_flat, batch_flat)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        current_step += 1\n",
    "\n",
    "        # ---------- VALIDATION ----------\n",
    "        if current_step % validation_mod == 0:\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for j in range(test_BT.shape[0] // batch_size):\n",
    "                    val_batch = test_BT[j * batch_size:(j + 1) * batch_size]\n",
    "\n",
    "                    val_pred = model(val_batch)          # [B, 4, V]\n",
    "\n",
    "                    val_batch_flat = val_batch.view(-1)  # [B*4]\n",
    "                    val_pred_flat = val_pred.view(-1, vocab_size)\n",
    "\n",
    "                    val_loss += criterion(val_pred_flat, val_batch_flat).item()\n",
    "\n",
    "                    preds = val_pred_flat.argmax(dim=1)\n",
    "                    correct += (preds == val_batch_flat).sum().item()\n",
    "                    total += val_batch_flat.numel()\n",
    "\n",
    "            val_loss /= (test_BT.shape[0] // batch_size)\n",
    "            val_acc = correct / total\n",
    "\n",
    "            print(\n",
    "                f\"Step: {current_step} | \"\n",
    "                f\"Train Loss: {loss.item():.4f} | \"\n",
    "                f\"Val Loss: {val_loss:.4f} | \"\n",
    "                f\"Val Acc: {val_acc:.4f}\"\n",
    "            )\n",
    "\n",
    "            model.train()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
